{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b594c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02defff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfile(filePath):\n",
    "    return filePath.split('\\\\')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de4f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitData(path):\n",
    "    columns = ['Center', 'Left', 'Right', 'Steering', 'Throttle', 'Brake', 'Speed']\n",
    "    data = pd.read_csv(os.path.join(path, \"driving_log.csv\"), names = columns)\n",
    "    data['Center'] = data['Center'].apply(getfile)\n",
    "    print('Total Images Imported', data.shape[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb381d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanceDataSet(data, display=True):\n",
    "    nBin = 21\n",
    "    samplesPerBin = 1000\n",
    "    hist, bins = np.histogram(data['Steering'], nBin)\n",
    "    if display:\n",
    "        center = (bins[:-1] + bins[1:]) * 0.5\n",
    "        plt.bar(center, hist, width=0.05)\n",
    "        plt.plot((np.min(data['Steering']), np.max(data['Steering'])), (samplesPerBin, samplesPerBin))\n",
    "        plt.show()\n",
    "\n",
    "    removeList = []\n",
    "    for i in range(nBin):\n",
    "        binDataList = []\n",
    "        for j in range(len(data['Steering'])):\n",
    "            if data['Steering'][j] >= bins[i] and data['Steering'][j] <= bins[i + 1]:\n",
    "                binDataList.append(j)\n",
    "        binDataList = shuffle(binDataList)\n",
    "        binDataList = binDataList[samplesPerBin:]\n",
    "        removeList.extend(binDataList)\n",
    "        \n",
    "    print('Removed Images: ', len(removeList))\n",
    "    data.drop(data.index[removeList], inplace=True)\n",
    "    print('Remaining Images: ', len(data))\n",
    "    \n",
    "    if display:\n",
    "        hist, _ = np.histogram(data['Steering'], (nBin))\n",
    "        plt.bar(center, hist,width=0.05)\n",
    "        plt.plot((np.min(data['Steering']), np.max(data['Steering'])), (samplesPerBin, samplesPerBin))\n",
    "        plt.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54c47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and steering angles\n",
    "def load_images_and_steering(data, img_folder='IMG'):\n",
    "    images = []\n",
    "    steerings = []\n",
    "    for i in range(len(data)):\n",
    "        img_path = os.path.join(img_folder, data.iloc[i]['Center'])\n",
    "        if os.path.isfile(img_path):\n",
    "            image = cv2.imread(img_path)\n",
    "            # images.append(image)\n",
    "            # steerings.append(float(data.iloc[i]['Steering']))\n",
    "            if image is not None and image.ndim == 3 and image.shape[2] == 3:\n",
    "                images.append(image)\n",
    "                steerings.append(float(data.iloc[i]['Steering']))\n",
    "            else:\n",
    "                print(f\"Skipping image {img_path}: invalid shape or None\")\n",
    "    return np.array(images), np.array(steerings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "183b64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess(img, index):\n",
    "    if img is None:\n",
    "        print(f\"[Preprocess Error] Image at index {index} is None\")\n",
    "        return None\n",
    "    if img.ndim != 3:\n",
    "        print(f\"[Preprocess Error] Image at index {index} has invalid ndim: {img.ndim}\")\n",
    "        return None\n",
    "    if img.shape[2] != 3:\n",
    "        print(f\"[Preprocess Error] Image at index {index} has shape: {img.shape}\")\n",
    "        return None\n",
    "    \n",
    "    # Crop image to Height[60:135] and the whole width\n",
    "    cropped_img = img[60:135, :] # [H:W]\n",
    "    \n",
    "    # Convert img to YUV color space\n",
    "    yuv_cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Resize to 200x66 (WxH)\n",
    "    resized_yuv = cv2.resize(yuv_cropped_img, (200, 66)) # (W, H)\n",
    "\n",
    "    # Normalize pixel values in [0:255] to [0:1] with division\n",
    "    resized_yuv_norm = resized_yuv / 255\n",
    "\n",
    "    # Blur with (5,5) kernel and inferred sigma\n",
    "    processed_img = cv2.GaussianBlur(resized_yuv_norm, (5,5), 0)\n",
    "\n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e40d2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(images, steerings, batch_size=32, shuffle_data=True):\n",
    "    \"\"\"\n",
    "    Creates batches of preprocessed images and steering angles\n",
    "    Args:\n",
    "        images: Preprocessed images array (n_samples, height, width, channels)\n",
    "        steerings: Corresponding steering angles (n_samples,)\n",
    "        batch_size: Number of samples per batch\n",
    "        shuffle_data: Whether to shuffle the data before batching\n",
    "    Returns:\n",
    "        Generator yielding (X_batch, y_batch) tuples\n",
    "    \"\"\"\n",
    "    num_samples = len(images)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        images, steerings = shuffle(images, steerings)\n",
    "    \n",
    "    # Calculate number of full batches\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    for batch_idx in range(0, num_batches * batch_size, batch_size):\n",
    "        X_batch = images[batch_idx:batch_idx + batch_size]\n",
    "        y_batch = steerings[batch_idx:batch_idx + batch_size]\n",
    "        yield X_batch, y_batch\n",
    "    \n",
    "    # Handle remaining samples (if any)\n",
    "    if num_samples % batch_size != 0:\n",
    "        X_batch = images[num_batches * batch_size:]\n",
    "        y_batch = steerings[num_batches * batch_size:]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d01f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driving_angle_model():\n",
    "    net = models.Sequential([\n",
    "                        # Original size passing the first CNN\n",
    "                        # With 24 filters, this extracts features such as edges and textures\n",
    "                        layers.Conv2D(filters=24, kernel_size=(5, 5), activation='relu', input_shape=(66,200,3)),\n",
    "                        \n",
    "                        # With 36 filters, this extracts deeper features such as lane curves\n",
    "                        layers.Conv2D(filters=36, kernel_size=(5, 5), activation='relu'),\n",
    "\n",
    "                        # With 48 layers, this extracts high level features such as lane patterns\n",
    "                        layers.Conv2D(filters=48, kernel_size=(5, 5), activation='relu'),\n",
    "\n",
    "                        # With 64 layers, this extracts fine grained features\n",
    "                        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "\n",
    "                        # Once again at 64 filters, this extracts the previous features once more\n",
    "                        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "\n",
    "                        # Flatten the result array to a 1D array to match size for Dense\n",
    "                        layers.Flatten(),\n",
    "\n",
    "                        # Separate 3 Dense compressions on result layer\n",
    "                        # Compressing with large number of neurons will overfit, and too low will underfit or lose info\n",
    "                        layers.Dense(100, activation='relu'),\n",
    "                        layers.Dense(50, activation='relu'),\n",
    "                        layers.Dense(1)\n",
    "                        ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "    # We want to determine the driving angle based on picture. Therefore, this is a regression problem\n",
    "    net.compile(optimizer, loss='mse')\n",
    "\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7de7170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images Imported 4221\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Images:  1546\n",
      "Remaining Images:  2675\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 3s/step - loss: 290596.4062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m net \u001b[38;5;241m=\u001b[39m driving_angle_model()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Fits the neural network with data\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Batch size between 2-32 can consistently be stable and reliable (source: Revisiting Small Batch Training for Deep Neural Networks)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Julian Huang\\OneDrive\\Desktop\\Seneca\\Visual Studio Code\\CVI620\\CVI620_virtual_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = InitData('.')\n",
    "data = balanceDataSet(data, display=True)\n",
    "\n",
    "images, steerings = load_images_and_steering(data)\n",
    "\n",
    "# processed_images = np.stack([img_preprocess(images) for img in images])\n",
    "\n",
    "processed_images = []\n",
    "for i, img in enumerate(images):\n",
    "    proc = img_preprocess(img, index=i)\n",
    "    if proc is not None:\n",
    "        processed_images.append(proc)\n",
    "\n",
    "processed_images = np.array(processed_images)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_images, steerings, test_size=0.2, random_state=42)\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "net = driving_angle_model()\n",
    "\n",
    "# Fits the neural network with data\n",
    "# Batch size between 2-32 can consistently be stable and reliable (source: Revisiting Small Batch Training for Deep Neural Networks)\n",
    "H = net.fit(aug.flow(X_train, y_train), validation_data=(X_test, y_test), batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(H.history[\"val_accuracy\"], label=\"test\")\n",
    "plt.plot(H.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(H.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVI620_ENV",
   "language": "python",
   "name": "cvi620_virtual_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
